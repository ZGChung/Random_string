import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

from colorama import Fore, Back, Style

# Prepare the data
examples = '''
00100001101110010101101000101001101000001111100110110001101110010110011010001001
11101111000010010010101101001001010101100011100111000011100010010011101110001001
00010111101010010011000101111001111111010101100100001001101010010110100100111001
01010101001010011110000011001001110111100110100110101101111010010001011100001001
10111000100010010001011101011001100010101001100110100101100110011000101011011001
11111101010010010101110110001001001110101010100100100100110110010001011000011001
11100011010010010001000001011001001011000011100101111010101010010101010110001001
11011110101110011110101110011001000111100001100111100000011110010100111110001001
11000011110110011110000000011001010111011101100101011101011010011110100100011001
01111101001110011010011101101001101100010001100101000011011010011100000011111001
00101100011110010100110000001001100011000111100110001000001110010000010110111001
11101100001110010111000011001001111001110100100101001101001110010011011001101001
10111110011110010001101000001001101011110001100110001001011010010000011101101001
11000010001010010100110001011001111010100011100100110000001010010010010110001001
10011111001010011011111001101001100100101001100111100001010010011010111100111001
01111011110010010111111001001001111010010101100111001011001010011010110000011001
00100101010010010100111100011001111000101110100100010111110110010110011100101001
10011110110110010001011111011001101101111011100110101100111010010001111110011001
11000001111010011011111011011001111101110100100100011111100010010101100101011001
00101001000110011010111101011001000010001101100100110011101010011101010111111001
00001100000110011100110010101001110010000100100101010100111110010000100100101001
01011100101010010010000011001001010001100001100110010010101010011101100011101001
11101001100010010001111101101001000000000110100111100011100010011001110010101001
11001111000010010110100110111001100100010000100101110001100110010001101101111001
10110001011010010001000111111001001001001000100101000001001010011111011100011001
10001110001010010110011111101001010010011001100111001100011110010111001001011001
11111110101010011000111010111001101000100111100110000011111010010101001001101001
11010100011110010011110111011001111100010100100111001110001110010111100110001001
10101100110010011101110101111001100000101101100110000100010110011110000110001001
01010110110110010101010010001001000010100100100110010110010010010100111010111001
00011001010010011110001000101001010001101010100100111011010110010011001000001001
01101111011010011101001100011001000001010101100101011111001010010110010010001001
11111010111010010110010001011001011100110100100110110100011010010000111101111001
00110110001110010000010011111001101010000111100111111110001110011010101011111001
10111110001110010010001101111001000110011100100110000100011010010111110010001001
11001111000110011011001100111001000000110010100110000001010010010001100110111001
01100101001110010011000001111001011110000110100110101000100010011111001000011001
10110101100010010101110001001001001000011000100100000101010110011010111100111001
11011000100110011001011000011001111000110000100101110000100010011001011000101001
00010001011110010001111011101001110010010100100101000001110010011001001000111001
00111000100010010001101000001001010000000111100110110010111010011000110100111001
00011111000110011111011000001001011011010010100111111100110010011000010110101001
11011101111010011100010111001001000001001000100100011000010110010110111101101001
10001100111110011100000001111001000001100000100100100010011110010001001011001001
00101000010110010100110000101001110100100100100101110111101110010111111101111001
00010111101010011101011001001001100101110000100110100111010110011011011110001001
00011100001010010101110110011001110000011111100100111110000010011011110110111001
11000011011010010010010101011001110111101010100110110011111010011100000101111001
01101010101010011000111010001001101111100010100110011100111010010110001001101001
00101100101010011011010001001001100001001011100111000000101110010100001110111001
01000001101110010011000111011001010001011101100111100110101010011011101010011001
10100001001010011100001001001001000100000100100100111100101110010101100100001001
01011110101110011111010000101001101011011011100101010101111110010110101000111001
10001010010010010011000101011001100110000001100111111100001010011101001101001001
01000110011110010000010111111001000110001100100100110110110010011100001111101001
11000101000010011001100001111001111010011000100111100101110010011100100000011001
00000011110110010011101010101001100110110001100111101110010010011011100111011001
01100111010110011111001001111001010100110101100110101011000010010000100011011001
01001111101110010001110001111001110010011110100110101101111010011111001111001001
10111010110110010100011111011001001000100101100100000101011010010101100010111001
10111110010110011111011011111001011111010100100101111101110110010010000000001001
00100100110010010011011100101001101111100101100111010011000110010111000011111001
10100011000010010110001000001001100011110100100111111001111010010101111000001001
00101010011010010110000011001001001001101101100101111011101110010111000001101001
01000111111110010000010111011001010011111001100100011110110110010111100100101001
10010010010110011011000010111001111110000001100100101111100010011111100101011001
10010101101110010011101100001001010011100101100111010100101010010111101000111001
10000100101010010000001100001001000011110111100100010010100010010000010111111001
11110111010010010100101011011001111110010100100100101010001110010110101110001001
10100110001010010101100010111001000110001100100110110001001010010000000001001001
00010000000010011010100010101001110100011111100110000001001010010010110010011001
10000111000110010010001111111001010011100000100111001010101110011011001011111001
10100110100110010011000010011001001111010011100100100101110110011010110100011001
01100011000110010011010110111001010011011001100101100010000010011001110100011001
01111011101110010011110110011001000011001111100100101001111110010101001111101001
01111000010010011001100110001001111100110001100111100111011110011101111001011001
11101100011110010011010000011001101100101011100101010101001110010011000010111001
11101101001010011111111000001001010101101001100111010111111110010110101011001001
01111000010010011000101000011001011000101010100110000111000010010010100011111001
00001110000010010010111011001001000110011111100111001111110010011111100101011001
10010010111010010010010010001001010110001001100101001001110110010100101111011001
00100100100010010010011110011001101000001011100101001010011110011100110110001001
00110011011010011000000000001001010010110110100101000000001010010011110101101001
01101010010110010101101010001001001011000100100100011110100010011110111000111001
11100111001010010111101100001001111010010110100100011110000110010111101101101001
10101001100110011000101111011001001001011001100101111010110010010001101100101001
11101110000010010111000110011001100110110111100100000010110110011101001001101001
01001000010110011011100000101001110101101010100100011101000010011101101111101001
00000000001010010100001010001001010000011100100101110110000110011100110001111001
11110100010110011110101110111001011111111011100100000111001110010001001000011001
10101110100110010110001001011001100011010000100100111111101010010011100101011001
00000010011110011111101100111001001001001111100100101101000010010110110001111001
00100011000110010000001000101001110010111100100101101111000110011101100100011001
00011001010110010100110111111001010100110100100110110000110110010111110111111001
10110010100010010010010100111001000111010101100100011011110110011011101100011001
01001001001010010011010000101001001001100100100101001010111010011101100101001001
10110000001010010111101001111001100011100000100101111100000010011011111100001001
11010010110010011110000001001001110111011000100101100000110010010100100100101001
10110011001010010100000100101001110001010110100110010010101110010110000111101001
01000101011110011001111101011001011010011111100110111010101110010010011111111001
'''



# Split examples into individual lines
lines = examples.split("\n")
lines = lines[:5]

# Split lines into individual digits
digits = [list(map(int, line)) for line in lines]

# Find the maximum length of digits
max_length = max(len(line) for line in digits)

# Pad lines with zeros if necessary
digits = [line + [0] * (max_length - len(line)) for line in digits]

# Reshape examples to have batch size and sequence length
batch_size = len(digits)
sequence_length = max_length // 4
examples = torch.tensor(digits, dtype=torch.float32).view(batch_size, sequence_length, 4)

# Define the LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, input_size)

    def forward(self, x):
        output, _ = self.lstm(x)
        output = self.fc(output)
        return output

# Set the input and hidden size
input_size = 4
hidden_size = 4

# Create the LSTM model instance
model = LSTMModel(input_size, hidden_size)

# Define the loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Create a DataLoader for the examples
dataset = TensorDataset(examples)
dataloader = DataLoader(dataset, batch_size=1)

# plot lists
epoch_list, loss_list, acc_list = [], [], []


# Training loop
max_epoch = 1000000
early_stop_count = 0
es_mark = 30
for epoch in range(max_epoch):
    for inputs in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs[0])
        loss = criterion(outputs, inputs[0])
        loss.backward()
        optimizer.step()

    if (epoch + 1) % 10 == 0:
        # eval
        # Generate predictions
        with torch.no_grad():
            predictions = model(examples)

        # Apply thresholding to convert predictions to binary values
        threshold = 0.5
        binary_predictions = (predictions >= threshold).int()
        res = binary_predictions.squeeze().tolist()
        total_mark = 5*len(lines)
        hit = 0
        for r in res:
            sub_d = ""
            for index, s in enumerate(r):
                if (index+1)%4==0:
                    if "".join([str(i) for i in s]) == '1001':
                        hit += 1
                    digits = "*"+"".join([str(i) for i in s])+"*"
                else:
                    digits = "".join([str(i) for i in s])
                sub_d += digits
                sub_d += ' '
        acc = hit/total_mark
        epoch_list.append(epoch+1)
        loss_list.append(loss.item())
        acc_list.append(acc)

        print(f'Epoch [{epoch + 1}/max_epoch], Loss: {loss.item():.4f}, Acc: {acc}')

        # early stop
        if float(acc) == 1.0:
            early_stop_count += 1
        if early_stop_count == es_mark:
            print("Early stop triggered.")
            break

        
        

        
# plot epoch_list, loss_list, acc_list
print(epoch_list)
print(loss_list)
print(acc_list)

import matplotlib.pyplot as plt

# Create the figure and axes
fig, ax1 = plt.subplots()

# Plot the first dataset (loss_list)
ax1.plot(epoch_list, loss_list, 'b-')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss', color='b')
ax1.tick_params('y', colors='b')

# Create a second y-axis
ax2 = ax1.twinx()

# Plot the second dataset (acc_list)
ax2.plot(epoch_list, acc_list, 'r-')
ax2.set_ylabel('Accuracy', color='r')
ax2.tick_params('y', colors='r')

# Set the title and show the plot
plt.title('Loss and Accuracy')
plt.savefig('lstm_loss_acc_grokking.png')



